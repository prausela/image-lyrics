{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pauli/.cache/pypoetry/virtualenvs/image-lyrics-hleqkuGI-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "def generate_caption(image_name):\n",
    "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "\n",
    "    # Path to the image in the images directory\n",
    "    img_path = os.path.join('docs/images', image_name)\n",
    "    raw_image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # conditional image captioning\n",
    "    text = \"a photography of\"\n",
    "    inputs = processor(raw_image, text, return_tensors=\"pt\")\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    print( processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "    # unconditional image captioning\n",
    "    inputs = processor(raw_image, return_tensors=\"pt\")\n",
    "\n",
    "    out = model.generate(**inputs)\n",
    "    return processor.decode(out[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ------------- Retrieval-Augmented Generation  ------------- #\n",
    "\n",
    "def get_docs():\n",
    "    \"\"\"\n",
    "    Loads each file into one document, like knowledge base\n",
    "    :return: docs\n",
    "    \"\"\"\n",
    "\n",
    "    loader = DirectoryLoader(\"docs/lyrics\", \"*.txt\", loader_cls=TextLoader)  # Reads custom data from local files\n",
    "\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "def split_text(docs):\n",
    "    \"\"\"\n",
    "    Get chunks from docs. Our loaded doc may be too long for most models, and even if it fits is can struggle to find relevant context. So we generate chunks\n",
    "    :param docs: docs to be split\n",
    "    :return: chunks\n",
    "    \"\"\"\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter( # recommended splitter for generic text\n",
    "        chunk_size=250,\n",
    "        chunk_overlap=25,\n",
    "        add_start_index=True\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_data_store(chunks, batch_size=1000):\n",
    "    \"\"\"\n",
    "    Store chunks into a db. ChromaDB uses vector embeddings as the key, creates a new DB from the documents\n",
    "    :param docs:\n",
    "    :param chunks:\n",
    "    :return: database\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings( #  embedding=OpenAIEmbeddings() rate limit\n",
    "        model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "        model_kwargs={'device': 'cpu'} # TODO gpu\n",
    "    )\n",
    "\n",
    "    db = None\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        if db is None:\n",
    "            db = Chroma.from_documents(documents=batch, embedding=embeddings)\n",
    "        else:\n",
    "            db.add_documents(batch)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20143/217949937.py:42: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings( #  embedding=OpenAIEmbeddings() rate limit\n",
      "/home/pauli/.cache/pypoetry/virtualenvs/image-lyrics-hleqkuGI-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, sys, warnings\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "docs = get_docs()           # Load custom files\n",
    "chunks = split_text(docs)   # Split into chunks\n",
    "db = get_data_store(chunks) # Generate vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RETRIEVED_CHUNKS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def generate_response(db, prompt):\n",
    "    \"\"\"\n",
    "    Generate a response with a LLM based on previous custom context\n",
    "    :return: chatbot response\n",
    "    \"\"\"\n",
    "\n",
    "    hf_llm = HuggingFaceHub(\n",
    "        repo_id=\"HuggingFaceH4/zephyr-7b-beta\",  # Model id\n",
    "        task=\"text-generation\",                  # Specific task the model is intended to perform\n",
    "        model_kwargs={\n",
    "            \"max_new_tokens\": 512,               # The maximum number of tokens to generate in the response.  Limits the length of the generated text to ensure responses are concise or fit within certain constraints.\n",
    "            \"top_k\": 30,                         # Limits the sampling pool to the top k tokens, increasing focus on more likely tokens\n",
    "            \"temperature\": 0.3,                  # Controls the randomness of predictions, with lower values making the output more deterministic. : Produces more focused and less random text by making the model more confident in its choices.\n",
    "            \"repetition_penalty\": 1.2,           # Penalizes repeated tokens to avoid repetitive output.  Discourages the model from repeating the same token sequences, resulting in more varied and natural text.\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type( # Generate chat model based on previous llm\n",
    "        llm=hf_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": N_RETRIEVED_CHUNKS}),\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    response = chain.run(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retrieved_chunks(response, retriever_k):\n",
    "    chosen_chunks = response.split('\\n\\n')\n",
    "    return chosen_chunks[1:retriever_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_response(response):\n",
    "    retrived_chunks = get_retrieved_chunks(response, N_RETRIEVED_CHUNKS)\n",
    "    answer_start = response.find(\"Helpful Answer: \")\n",
    "    if answer_start != -1:\n",
    "        answer = response[answer_start + len(\"Helpful Answer: \"):].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "\n",
    "    return answer, retrived_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewriting(response):\n",
    "    answer_start = response.find(\"araf\")\n",
    "    if answer_start != -1:\n",
    "        answer = response[answer_start + len(\"araful\"):].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hola! Por favor inserte el nombre de la imagen que quieras usar para escribir una canci칩n. Las im치genes se encuentran dentro de la carpeta /docs/. Escribe done cuando termines.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pauli/.cache/pypoetry/virtualenvs/image-lyrics-hleqkuGI-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/home/pauli/.cache/pypoetry/virtualenvs/image-lyrics-hleqkuGI-py3.10/lib/python3.10/site-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a lake with a mountain in the background\n",
      "Chatbot: Intro:\n",
      "Amidst the crisp autumn air,\n",
      "The mountain stands tall and proud,\n",
      "A tranquil lake nestled at its base,\n",
      "Red leaves dancing in the wind like a crowd. Verse 1:\n",
      "As I gaze upon that distant sight,\n",
      "My heart skips a beat or two,\n",
      "Memories flood back in an instant,\n",
      "Of times when life was brand new. Chorus 1:\n",
      "Oh, how the beauty takes my breath away,\n",
      "This scene etched deep within my soul,\n",
      "With every passing moment,\n",
      "It grows more radiant, more whole. Verse 2:\n",
      "Walking along the shoreline's edge,\n",
      "Leaves rustling underfoot,\n",
      "Whispers echoing off the stillness,\n",
      "Nature's symphony, pure and moot. Chorus 2:\n",
      "Here where time seems to slow,\n",
      "Each step taken is a gift,\n",
      "For in these moments, nothing else matters,\n",
      "Only the mountain, the lake, and the shift. Bridge:\n",
      "Letting go of what once held sway,\n",
      "Embracing change with open arms,\n",
      "Surrendering to the ebb and flow,\n",
      "Of nature's rhythmic charms. Chorus 3:\n",
      "May this place always hold such power,\n",
      "Guiding me towards peace and calm,\n",
      "May the mountain, the lake, and the leaves,\n",
      "Be my constant source of balm. Outro:\n",
      "Until next time, farewell for now,\n",
      "May the memories linger long,\n",
      "Till we meet again, in spirit or form,\n",
      "Underneath the mountain's strong.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hola! Por favor inserte el nombre de la imagen que quieras usar para escribir una canci칩n. Las im치genes se encuentran dentro de la carpeta /docs/. Escribe done cuando termines.\")\n",
    "#user_input = \"\"\n",
    "user_input = input(\"You: \")\n",
    "cap = \"\"\n",
    "while user_input != \"done\":\n",
    "    cap += query_rewriting(generate_caption(user_input))\n",
    "    user_input = input(\"You: \")\n",
    "\n",
    "response = generate_response(db, f\"Write a song lyrics about \\\"{cap}\\\" having intro, verse 1, chorus 1, verse 2, chorus 2, bridge, chorus 3, outro. Ignore the words intro, verse, chorus and outro.\")\n",
    "out, chunks = postprocess_response(response)\n",
    "print(f\"Chatbot: {out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[Intro]\\nPlease, please tell me now\\nPlease, please tell me now\\nPlease, please tell me now\\nPlease, please tell me now\\n[Verse 1]\\nI made a break, I run out yesterday\\nTried to find my mountain hideaway\\nMaybe next year, maybe no go',\n",
       " \"Mountains come out of the sky and they stand there\\nOne mile over we'll be there and we'll see you\\nTen true summers we'll be there and laughing tooTwenty four before my love you'll see\\nI'll be there with you\\n[Outro with Vocalizations]\",\n",
       " \"[Bridge]\\n(Hoo hoo hoo)\\n[Verse 2]Reach your hand down into the cooler\\nDon't drink it if the mountains aren't blueTry to keep it steady as you recline on your black inner tube\\n[Chorus]\\n[Bridge]\\n(Hoo hoo hoo)\\n(Hoo hoo hoo)\",\n",
       " \"Cause I'm counting on\\nA new beginning, a reason for living\\nA deeper meaning, yeah\\n[Chorus]\\nI want to stand with you on a mountain\\nI want to bathe with you in the sea\\nI want to lay like this forever\\nUntil the sky falls down on me\",\n",
       " 'There are hills and mountains between us\\nAlways something to get over\\nIf I had my way, then surely you would be closer\\nI need you closer\\n(Musical break)\\nThere are hills and mountains between us\\nAlways something to get over',\n",
       " \"[Chorus]\\nOh-oh-oh-oh-oh-oh-oh-oh-oh-oh-oh\\nOh-oh-oh-oh-oh-oh-oh-oh-oh-oh-oh\\nOh-oh-oh-oh-oh-oh-oh-oh-oh-oh-oh\\nOh-oh-oh-oh-oh-oh-oh-oh-oh-oh-oh\\n[Verse 1]\\nJungle life, I'm far away from nowhere\\nOn my own, like Tarzan, boy\",\n",
       " \"I spend the day your way\\nCall it morning driving through the sound and\\nIn and out the valley\\n[Chorus 2]In and around the lake\\nMountains come out of the sky and they stand there\\nOne mile over we'll be there and we'll see you\",\n",
       " '[Chorus]\\nSmoke on the water and fire in the skySmoke on the water',\n",
       " \"[Chorus]\\nDeep beneath the cover of another perfect wonder\\nWhere it's so white as snow\\nPrivately divided by a world so undecidedAnd there's nowhere to go\\nIn between the cover of another perfect wonder\\nWhere it's so white as snow\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-lyrics-fA5n5Nhs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
