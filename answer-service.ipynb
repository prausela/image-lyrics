{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def generate_response(db, prompt):\n",
    "    \"\"\"\n",
    "    Generate a response with a LLM based on previous custom context\n",
    "    :return: chatbot response\n",
    "    \"\"\"\n",
    "\n",
    "    hf_llm = HuggingFaceHub(\n",
    "        repo_id=\"HuggingFaceH4/zephyr-7b-beta\",  # Model id\n",
    "        task=\"text-generation\",                  # Specific task the model is intended to perform\n",
    "        model_kwargs={\n",
    "            \"max_new_tokens\": 512,               # The maximum number of tokens to generate in the response.  Limits the length of the generated text to ensure responses are concise or fit within certain constraints.\n",
    "            \"top_k\": 30,                         # Limits the sampling pool to the top k tokens, increasing focus on more likely tokens\n",
    "            \"temperature\": 0.3,                  # Controls the randomness of predictions, with lower values making the output more deterministic. : Produces more focused and less random text by making the model more confident in its choices.\n",
    "            \"repetition_penalty\": 1.2,           # Penalizes repeated tokens to avoid repetitive output.  Discourages the model from repeating the same token sequences, resulting in more varied and natural text.\n",
    "        },\n",
    "    )\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type( # Generate chat model based on previous llm\n",
    "        llm=hf_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2}),\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    response = chain.run(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_response(response):\n",
    "    answer_start = response.find(\"Helpful Answer: \")\n",
    "    if answer_start != -1:\n",
    "        answer = response[answer_start + len(\"Helpful Answer: \"):].strip()\n",
    "    else:\n",
    "        answer = response.strip()\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocess_response(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
